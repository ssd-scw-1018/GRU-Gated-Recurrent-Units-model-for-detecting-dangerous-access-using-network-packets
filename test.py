# test.py
from pathlib import Path
from collections import defaultdict, deque
import numpy as np
import pandas as pd
import torch
import shutil 
import os
import torch.nn as nn

# ì‹œê°í™”ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# ================== ì„¤ì • ë° ìƒìˆ˜ ==================
ART = Path("artifacts_parquet")

# ðŸ”¹ fake íŒŒì¼ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ (ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„œë¸Œí´ë” ìƒì„± ì˜ˆì •)
FAKE_ROOT = Path("fake_files")

# [ì„¤ì •] ê°€ì§œ íŒŒì¼ì„ ìµœëŒ€ ëª‡ ê°œê¹Œì§€ ìœ ì§€í•  ê²ƒì¸ê°€?
MAX_FAKE_FILES = 10

# [ì„¤ì •] ë™ì¼ ë¡œê·¸ ë°˜ë³µ ì¶œë ¥ ì œí•œ íšŸìˆ˜
LOG_REPEAT_LIMIT = 5

# [ì„¤ì •] ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœëŒ€ ë¡œê·¸ ì¶œë ¥ ì¤„ ìˆ˜
LOG_MAX_LINES = 100

plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

def list_scenario_splits():
    """
    ART ë””ë ‰í† ë¦¬ ì•ˆì˜ *_X.parquet ì¤‘ì—ì„œ
    - test_X.parquet ì€ ì œì™¸í•˜ê³ 
    - ë‚˜ë¨¸ì§€ íŒŒì¼ë“¤ì˜ prefix( *_X ì•ž ë¶€ë¶„ )ë¥¼ ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ìœ¼ë¡œ ì¸ì‹í•œë‹¤.

    ì˜ˆ)
      scenario_ddos_X.parquet   -> "scenario_ddos"
      scenario_slow_scan_X.parquet -> "scenario_slow_scan"
      scenario_missing_X.parquet   -> "scenario_missing"
    """
    splits = []
    for p in ART.glob("*_X.parquet"):
        stem = p.stem             # ì˜ˆ: 'scenario_ddos_X', 'test_X'
        if stem == "test_X":
            # test_X.parquetì€ â€œí•™ìŠµ/í‰ê°€ìš© ì›ë³¸ í…ŒìŠ¤íŠ¸ì…‹â€ì´ë¼ ì‹œë‚˜ë¦¬ì˜¤ ì•„ë‹˜
            continue

        # ë’¤ì˜ "_X" ë–¼ê¸°
        if stem.endswith("_X"):
            split_name = stem[:-2]  # 'scenario_ddos_X' -> 'scenario_ddos'
        else:
            split_name = stem

        if split_name == "test":
            # í˜¹ì‹œë¼ë„ 'test_X'ë¥¼ ë˜ ìž¡ì•„ë„ ë°©ì–´
            continue

        splits.append(split_name)

    # ì •ë ¬(ì„ íƒ ì‚¬í•­)
    splits = sorted(set(splits))
    print(f"[info] ë°œê²¬ëœ ì‹œë‚˜ë¦¬ì˜¤ splits: {splits}")
    return splits
import json

def data_check_before_run():
    """
    test_XëŠ” ê²€ì‚¬í•˜ì§€ ì•ŠìŒ.
    ART ì•ˆì— ìžˆëŠ” *_X.parquet ì¤‘ test_Xë¥¼ ì œì™¸í•œ
    ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°ì— ëŒ€í•´:

      - core_features.json ê¸°ë°˜ í•µì‹¬ í”¼ì²˜ ì¡´ìž¬ ì—¬ë¶€
      - í•µì‹¬ í”¼ì²˜ ê²°ì¸¡ë¥  (ê°€ëŠ¥í•˜ë©´ raw JSON ê¸°ì¤€)

    ì„ ê²€ì‚¬í•˜ê³ , ê¸°ì¤€ì„ í†µê³¼í•œ ì‹œë‚˜ë¦¬ì˜¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤.
    """
    print("\n========== [DATA CHECK: SCENARIO DATA] ==========")

    # 0) í˜„ìž¬ ì¡´ìž¬í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ split ìžë™ ìˆ˜ì§‘
    scenario_splits = list_scenario_splits()
    if not scenario_splits:
        print("[!] ì‹œë‚˜ë¦¬ì˜¤ìš© *_X.parquet íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return []

    # 1) í•µì‹¬ í”¼ì²˜ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
    core_path = Path("core_features.json")
    if not core_path.exists():
        alt_path = ART / "core_features.json"
        if alt_path.exists():
            core_path = alt_path
        else:
            print("[!] core_features.json ì—†ìŒ â†’ ì¤‘ìš” í”¼ì²˜ ê¸°ë°˜ ì²´í¬ ë¶ˆê°€.")
            print("    feature_importance_auc.py ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì„œ core_features.jsonì„ ìƒì„±í•˜ì„¸ìš”.")
            return []

    print(f" - using core_features.json from: {core_path}")
    core = json.load(open(core_path, "r", encoding="utf-8"))
    core_features = core.get("core_features", [])
    if not core_features:
        print("[!] core_features.json ì•ˆì— 'core_features' í‚¤ê°€ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤.")
        return []

    print(f" - í•µì‹¬ í”¼ì²˜ {len(core_features)}ê°œ ë¡œë“œë¨")
    print(f"   {core_features}")

    CRITICAL_THRESHOLD = 0.05  # í•µì‹¬ í”¼ì²˜ ê²°ì¸¡ë¥  í—ˆìš© ìµœëŒ€ê°’ (5%)

    valid_scenarios = []
    invalid_scenarios = []

    # ---------------------------
    # ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ ê²€ì‚¬
    # ---------------------------
    for split in scenario_splits:
        split_ok = True

        x_path = ART / f"{split}_X.parquet"
        print(f"\n--- Checking scenario: {split} ({x_path.name}) ---")

        if not x_path.exists():
            print(f"[!] {x_path.name} ì—†ìŒ â†’ ì´ ì‹œë‚˜ë¦¬ì˜¤ëŠ” ìŠ¤í‚µë¨")
            split_ok = False
            invalid_scenarios.append(split)
            continue

        df = pd.read_parquet(x_path)
        df = df.drop(columns=["event_id"], errors="ignore")

        # 1) í•µì‹¬ í”¼ì²˜ ì¡´ìž¬ ì—¬ë¶€ (ëª¨ë¸ì´ ì‹¤ì œë¡œ ì“¸ ìˆ˜ ìžˆëŠ”ì§€ í™•ì¸)
        missing_features = [c for c in core_features if c not in df.columns]
        if missing_features:
            print(f"[X] í•µì‹¬ í”¼ì²˜ ëˆ„ë½ â†’ {missing_features}")
            split_ok = False

        else:
            print(" - í•µì‹¬ í”¼ì²˜ ì¡´ìž¬ OK")

            # 2) ê²°ì¸¡ë¥  ì²´í¬
            #    2-1) raw ê²°ì¸¡ë¥  JSONì´ ìžˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©
            raw_json_path = ART / f"{split}_raw_missing.json"
            if raw_json_path.exists():
                raw_info = json.load(open(raw_json_path, "r", encoding="utf-8"))
                raw_missing = raw_info.get("missing_rate", {})
                use_raw = True
                print(f"   (raw missing ì‚¬ìš©: {raw_json_path.name})")
            else:
                # ì—†ìœ¼ë©´ ì „ì²˜ë¦¬ í›„ íŒŒì¼“ ê¸°ì¤€ìœ¼ë¡œë¼ë„ ì²´í¬ (fallback)
                miss_rate_series = df.isna().mean()
                raw_missing = {c: float(r) for c, r in miss_rate_series.items()}
                use_raw = False
                print("   (raw JSON ì—†ìŒ â†’ parquet ê¸°ì¤€ ê²°ì¸¡ë¥  ì‚¬ìš©)")

            for f in core_features:
                if use_raw:
                    # raw_missing ì— ì—†ìœ¼ë©´ "ì—´ì´ ì•„ì˜ˆ ì—†ì—ˆë‹¤"ë¡œ ë³´ê³  100% ê²°ì¸¡ìœ¼ë¡œ ì·¨ê¸‰
                    r = float(raw_missing.get(f, 1.0))
                else:
                    r = float(raw_missing.get(f, 0.0))

                if r > CRITICAL_THRESHOLD:
                    print(f"[!] í•µì‹¬ í”¼ì²˜ '{f}' ê²°ì¸¡ë¥  = {r*100:.2f}% "
                          f"(í—ˆìš© {CRITICAL_THRESHOLD*100:.1f}% ì´ˆê³¼)")
                    split_ok = False
                else:
                    print(f" - {f}: ê²°ì¸¡ë¥  {r*100:.2f}% OK")

        if split_ok:
            print(f" --> âœ… scenario '{split}' ì‚¬ìš© ê°€ëŠ¥")
            valid_scenarios.append(split)
        else:
            print(f" --> âš  scenario '{split}' ëŠ” ê¸°ì¤€ ë¯¸ë‹¬ (ì‹¤í–‰ ëŒ€ìƒì—ì„œ ì œì™¸)")
            invalid_scenarios.append(split)

    # ìš”ì•½ ì¶œë ¥
    print("\n[DATA CHECK SUMMARY]")
    print(f" - ì‚¬ìš© ê°€ëŠ¥ ì‹œë‚˜ë¦¬ì˜¤: {valid_scenarios}")
    print(f" - ì œì™¸ëœ ì‹œë‚˜ë¦¬ì˜¤  : {invalid_scenarios}")

    if not valid_scenarios:
        print("[!] ê¸°ì¤€ì„ í†µê³¼í•œ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # âœ… ì´ì œ boolì´ ì•„ë‹ˆë¼ "ì“¸ ìˆ˜ ìžˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ ë¦¬ìŠ¤íŠ¸"ë¥¼ ë°˜í™˜
    return valid_scenarios

# ================= Model ì •ì˜ =================
class GRUCls(nn.Module):
    def __init__(self, in_dim, hid=128, num_layers=1, dropout=0.3, n_classes=2):
        super().__init__()
        self.gru = nn.GRU(
            in_dim, hid,
            num_layers=num_layers,
            batch_first=True,
            dropout=(dropout if num_layers > 1 else 0.0)
        )
        self.bn = nn.BatchNorm1d(hid)
        self.fc = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hid, n_classes)
        )

    def forward(self, x):
        out, _ = self.gru(x)
        last = self.bn(out[:, -1, :])
        return self.fc(last)

def decide_dual(p, lo, hi):
    p = np.asarray(p)
    return np.where(p < lo, 0, np.where(p >= hi, 2, 1))

# ================== í•µì‹¬ ë³´ì•ˆ ì—”ì§„ ==================
class SecurityPolicyEngine:
    def __init__(self, model, tau_lo, tau_hi, device,
                 fake_dir: Path, watch_threshold=5):
        """
        fake_dir: ì´ ì¸ìŠ¤í„´ìŠ¤(ì‹œë‚˜ë¦¬ì˜¤)ì—ì„œ ì‚¬ìš©í•  fake íŒŒì¼ ì „ìš© ë””ë ‰í† ë¦¬
                  ì˜ˆ: fake_files/scenario_ddos, fake_files/scenario_slow_scan ...
        """
        self.model = model
        self.tau_lo = tau_lo
        self.tau_hi = tau_hi
        self.device = device
        self.watch_threshold = watch_threshold

        self.block_list = set()
        self.watch_counts = defaultdict(int)
        self.fake_file_queue = deque()
        self.dummy_content = os.urandom(1024)  # 1KB ë”ë¯¸ ë°ì´í„°

        # ì‹œë‚˜ë¦¬ì˜¤ë³„ fake ë””ë ‰í† ë¦¬
        self.fake_dir = fake_dir

        self._init_environment()

    def _init_environment(self):
        """í•´ë‹¹ ì‹œë‚˜ë¦¬ì˜¤ ì „ìš© fake_dirë§Œ ì •ë¦¬"""
        if self.fake_dir.exists():
            try:
                shutil.rmtree(self.fake_dir, ignore_errors=True)
            except Exception:
                pass
        self.fake_dir.mkdir(parents=True, exist_ok=True)
            
    def _create_dynamic_fake_file(self, requested_fname):
        """í•´ë‹¹ ì‹œë‚˜ë¦¬ì˜¤ ì „ìš© ë””ë ‰í† ë¦¬ì— fake íŒŒì¼ ìƒì„±"""
        target_path = self.fake_dir / requested_fname

        if target_path.exists():
            return str(target_path)

        while len(self.fake_file_queue) >= MAX_FAKE_FILES:
            oldest = self.fake_file_queue.popleft()
            try:
                os.remove(self.fake_dir / oldest)
            except OSError:
                pass

        try:
            with open(target_path, "wb") as f:
                f.write(self.dummy_content)
            self.fake_file_queue.append(requested_fname)
            return str(target_path)
        except Exception:
            return str(target_path)

    def _predict(self, x_seq):
        arr = np.asarray(x_seq, dtype=np.float32)
        if arr.ndim == 1:
            arr = arr[None, None, :]
        elif arr.ndim == 2:
            arr = arr[:, None, :]
        x_tensor = torch.tensor(arr, dtype=torch.float32, device=self.device)
        with torch.no_grad():
            logits = self.model(x_tensor)
            prob = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()[0]
        return float(prob)

    def decide(self, ip, f_name, x_seq):
        # 1. ì°¨ë‹¨ëœ IP
        if ip in self.block_list:
            return "BLOCKED", None, None, {
            "watch_count": self.watch_counts.get(ip, 0),
            "is_alert_ip": True
            }

        # 2. ëª¨ë¸ ì¶”ë¡ 
        prob = self._predict(x_seq)
        code = decide_dual(np.array([prob]), self.tau_lo, self.tau_hi)[0]

        # 3. ì •ì±… ì ìš©
        if code == 2:  # ALERT
            self.block_list.add(ip)
            fake_path = self._create_dynamic_fake_file(f_name)
            return "BLOCKED", fake_path, prob, {
                "watch_count": self.watch_counts.get(ip, 0),
                "is_alert_ip": True
            }

        elif code == 1:  # WATCH
            self.watch_counts[ip] += 1
            if self.watch_counts[ip] >= self.watch_threshold:
                self.block_list.add(ip)
                fake_path = self._create_dynamic_fake_file(f_name)
                return "BLOCKED", fake_path, prob, {
                    "watch_count": self.watch_counts[ip],
                    "is_alert_ip": True
                }
            else:
                return "WATCH", f"real_files/{f_name}", prob, {
                    "watch_count": self.watch_counts[ip],
                    "is_alert_ip": False
                }

        else:  # NORMAL
            return "NORMAL", f"real_files/{f_name}", prob, {
                "watch_count": self.watch_counts.get(ip, 0),
                "is_alert_ip": False
            }

# ================== ì‹œê°í™” ìœ í‹¸ ==================
def load_data_for_vis():
    RESULT_CSV = ART / "test_decisions.csv"
    LABEL_DATA = ART / "scenario_y.parquet"
    if not RESULT_CSV.exists():
        print("ê²°ê³¼ íŒŒì¼(test_decisions.csv)ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    df_pred = pd.read_csv(RESULT_CSV)
    if LABEL_DATA.exists():
        df_label = pd.read_parquet(LABEL_DATA)
        df = pd.merge(df_pred, df_label, on="event_id", how="left")
    else:
        df = df_pred
        print("Warning: scenario_y.parquetê°€ ì—†ì–´ ì •ë‹µ ë¹„êµ(í˜¼ë™ í–‰ë ¬)ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.")
    return df

def plot_ip_based_summary(df, title_suffix=""):
    if df.empty:
        return
    ip_states = {}
    for ip, g in df.groupby("ip"):
        decisions = set(g["decision"])
        if "BLOCKED" in decisions:
            ip_states[ip] = "BLOCKED"
        elif "WATCH" in decisions:
            ip_states[ip] = "WATCH"
        else:
            ip_states[ip] = "NORMAL"

    state_counts = pd.Series(list(ip_states.values())).value_counts()
    colors = {'NORMAL': '#2ecc71', 'WATCH': '#f1c40f', 'BLOCKED': '#e74c3c'}
    col_list = [colors.get(x, '#95a5a6') for x in state_counts.index]

    plt.figure(figsize=(8, 6))
    plt.pie(
        state_counts, labels=state_counts.index, autopct='%1.1f%%',
        startangle=140, colors=col_list
    )
    plt.title(f"IP ê¸°ì¤€ ë³´ì•ˆ ìƒíƒœ ë¶„í¬{title_suffix}")
    plt.tight_layout()
    plt.show()

def plot_attack_scenario(df, target_ip, tau_lo, tau_hi, title_suffix=""):
    subset = df[df['ip'] == target_ip].copy()
    if subset.empty:
        return

    # event_id ê¸°ì¤€ìœ¼ë¡œ ì‹œê°„ ìˆœ ì •ë ¬
    subset = subset.sort_values('event_id').reset_index(drop=True)

    # ðŸ”¹ xì¶•ìœ¼ë¡œ ì“¸ ì ‘ê·¼ ìˆœì„œ(0,1,2,...) ìƒì„±
    subset['seq_idx'] = np.arange(len(subset))

    plt.figure(figsize=(12, 6))
    # íšŒìƒ‰ ì„ : ì „ì²´ ê¶¤ì 
    plt.plot(
        subset['seq_idx'], subset['prob'],
        label='Attack Probability', color='gray', alpha=0.5
    )

    states = subset['decision'].unique()
    markers = {'NORMAL': 'o', 'WATCH': 'v', 'BLOCKED': 'X'}
    colors  = {'NORMAL': 'green', 'WATCH': 'orange', 'BLOCKED': 'red'}

    for state in states:
        mask = subset['decision'] == state
        plt.scatter(
            subset.loc[mask, 'seq_idx'], subset.loc[mask, 'prob'],
            label=state, marker=markers.get(state, 'o'),
            c=colors.get(state, 'blue'), s=60
        )

    plt.axhline(
        y=tau_hi, color='r', linestyle='--', alpha=0.5,
        label=f'Alert Threshold ({tau_hi:.3f})'
    )
    plt.axhline(
        y=tau_lo, color='y', linestyle='--', alpha=0.5,
        label=f'Watch Threshold ({tau_lo:.3f})'
    )

    plt.title(f"ê³µê²© ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ - IP: {target_ip}{title_suffix}")
    plt.xlabel("Event Order in Scenario (seq_idx)")  # ðŸ”¹ xì¶• ì´ë¦„
    plt.ylabel("Malicious Probability")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()


def plot_confusion_matrix(df, title_suffix=""):
    if 'Label' not in df.columns:
        return

    y_true = df['Label']

    # BLOCKED ë§Œ íƒì§€ë¡œ ì¸ì •
    y_pred = df['decision'].apply(lambda x: 1 if x == 'BLOCKED' else 0)

    print("\n" + "="*40)
    print(f" [System Performance Report - BLOCKED only]{title_suffix}")
    print("="*40)
    print(classification_report(
        y_true, y_pred,
        target_names=['Normal', 'Blocked']
    ))

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(
        cm, annot=True, fmt='d', cmap='Greens',
        xticklabels=['Pred Normal/Watch', 'Pred Block'],
        yticklabels=['True Normal', 'True Attack']
    )
    plt.title(f"ë³´ì•ˆ ì •ì±… ì ìš© í›„ í˜¼ë™ í–‰ë ¬ (BLOCKED only){title_suffix}")
    plt.ylabel("ì‹¤ì œ ê°’ (True)")
    plt.xlabel("ì‹œìŠ¤í…œ íŒë‹¨ (Pred)")
    plt.tight_layout()
    plt.show()

def pick_most_interesting_ip(df):
    """
    ê°€ìž¥ ë³¼ë§Œí•œ IPë¥¼ ì„ íƒí•œë‹¤.
    ê¸°ì¤€ = NORMAL / WATCH / BLOCKED ì‚¬ì´ì˜ ìƒíƒœ ì „ì´ê°€ ê°€ìž¥ ë§Žì€ IP
    """
    transition_scores = {}

    for ip, g in df.groupby("ip"):
        g = g.sort_values("event_id")
        states = g["decision"].tolist()

        # ì—°ì†ëœ ìƒíƒœê°€ ë°”ë€ íšŸìˆ˜ ê³„ì‚°
        transitions = sum(1 for i in range(1, len(states)) if states[i] != states[i-1])

        transition_scores[ip] = transitions

    if not transition_scores:
        return None

    # ë³€í™”ëŸ‰ì´ ê°€ìž¥ í° IPë¥¼ ì„ íƒ
    return max(transition_scores, key=transition_scores.get)
def run_visualization(tau_lo, tau_hi):
    print("\n[+] ì‹œê°í™” ë° ë¦¬í¬íŠ¸ ìƒì„±ì„ ì‹œìž‘í•©ë‹ˆë‹¤...")
    df_all = load_data_for_vis()
    if df_all is None:
        return

    for scen in df_all["scenario"].unique():
        print(f"\n===== [Scenario: {scen}] =====")
        df = df_all[df_all["scenario"] == scen].copy()
        suffix = f" ({scen})"

        # IP ê¸°ì¤€ Pie Chart
        plot_ip_based_summary(df, title_suffix=suffix)

        # ê³µê²© ì‹œë‚˜ë¦¬ì˜¤ ê·¸ëž˜í”„
        target_ip = pick_most_interesting_ip(df)

        if target_ip is not None:
            print(f"[Graph] '{scen}'ì—ì„œ ë³€í™”ëŸ‰ì´ ê°€ìž¥ ë§Žì€ IP '{target_ip}'ë¥¼ ì„ íƒí•˜ì—¬ ê·¸ëž˜í”„ ìƒì„±")
            plot_attack_scenario(df, target_ip, tau_lo, tau_hi, title_suffix=suffix)
        else:
            print(f"[Info] '{scen}'ì—ì„œ ì ì ˆí•œ IPë¥¼ ì°¾ì§€ ëª»í•´ ê·¸ëž˜í”„ë¥¼ í‘œì‹œí•˜ì§€ ì•ŠìŒ")


        # í˜¼ë™ í–‰ë ¬
        plot_confusion_matrix(df, title_suffix=suffix)

# ================== ë°ì´í„° ë¡œë” ==================
def build_events_with_meta(split="test"):
    X = pd.read_parquet(ART / f"{split}_X.parquet")
    meta = pd.read_parquet(ART / f"{split}_meta.parquet")

    ev = X["event_id"].astype("int64").to_numpy()
    feats = X.drop(columns=["event_id"]).to_numpy(np.float32)

    if meta.index.name != "event_id":
        meta = meta.set_index("event_id")

    srcip_arr = meta.loc[ev, "srcip"].astype(str).to_numpy()

    if "f_name" in meta.columns:
        fname_arr = meta.loc[ev, "f_name"].astype(str).to_numpy()
    else:
        fname_arr = np.array(["unknown.dat"] * len(ev), dtype=object)

    X_seq = feats[:, None, :].astype(np.float32)
    print(f"[build_events_with_meta] split={split}, X_seq={X_seq.shape}")
    return X_seq, srcip_arr, fname_arr, ev

# ================== ëª¨ë¸ ë¡œë“œ ==================
def load_model(ckpt_path="gru_dual_threshold_model.pth"):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)

    config = ckpt["config"]
    model = GRUCls(
        in_dim=config["in_dim"],
        hid=config["hid"],
        num_layers=config["num_layers"],
        dropout=config["dropout"],
        n_classes=config["n_classes"],
    ).to(device)

    model.load_state_dict(ckpt["model_state_dict"])
    model.eval()

    tau_lo = float(ckpt["thresholds"]["tau_lo"])
    tau_hi = float(ckpt["thresholds"]["tau_hi"])

    print(f"[+] model loaded on {device}, tau_lo={tau_lo:.3f}, tau_hi={tau_hi:.3f}")
    return model, tau_lo, tau_hi, device

# ================== ë©”ì¸ ì‹¤í–‰ë¶€ ==================
# ================== ë©”ì¸ ì‹¤í–‰ë¶€ ==================
def run_on_test_split(scenario_splits=None):
    # data_checkì—ì„œ ì´ë¯¸ í•„í„°ë§ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë„˜ê²¨ì¤„ ìˆ˜ë„ ìžˆê³ ,
    # ì§ì ‘ í˜¸ì¶œí•  ë•ŒëŠ” Noneìœ¼ë¡œ ë‘ë©´ ì „ì²´ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
    if scenario_splits is None:
        scenario_splits = list_scenario_splits()

    if not scenario_splits:
        print("[!] ì‹¤í–‰í•  ì‹œë‚˜ë¦¬ì˜¤ splitì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    model, tau_lo, tau_hi, device = load_model()
    all_logs = []

    for split_name in scenario_splits:
        print("\n" + "=" * 60)
        print(f"[+] Starting Traffic Analysis on scenario: {split_name}")
        print("=" * 60)
        
        # ðŸ”¹ ì‹œë‚˜ë¦¬ì˜¤ë³„ fake íŒŒì¼ ë””ë ‰í† ë¦¬ (fake_files/scenario_xxx)
        scenario_fake_dir = FAKE_ROOT / split_name

        engine = SecurityPolicyEngine(
            model, tau_lo, tau_hi, device,
            fake_dir=scenario_fake_dir,
            watch_threshold=5,
        )

        X_seq, seq_ips, seq_fnames, seq_eids = build_events_with_meta(split_name)
        decisions_log = []

        # ë¡œê·¸ ì¤‘ë³µ ì¶œë ¥ ë°©ì§€ìš© ë³€ìˆ˜
        prev_ip = None
        prev_decision = None
        consecutive_count = 0

        # ðŸ”¹ ì´ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì‹¤ì œ ì½˜ì†”ì— ì°ížŒ ì¤„ ìˆ˜
        printed_lines = 0
        log_truncated = False
        
        for i in range(len(X_seq)):
            ip = seq_ips[i]
            f_name = seq_fnames[i]
            x_seq = X_seq[i]
            decision, served_path, prob, state = engine.decide(ip, f_name, x_seq)

            decisions_log.append({
                "scenario": split_name,
                "event_id": int(seq_eids[i]),
                "ip": ip,
                "f_name": f_name,
                "served_file": served_path,
                "decision": decision,
                "prob": prob,
                "watch_count": state["watch_count"],
                "is_alert_ip": state["is_alert_ip"],
            })

            # -----------------------------
            # ë¡œê·¸ ì¶œë ¥ ë¡œì§ (ì¤‘ë³µ ìš”ì•½ + ìµœëŒ€ ì¤„ ìˆ˜ ì œí•œ)
            # -----------------------------
            if ip != prev_ip or decision != prev_decision:
                # ìƒíƒœê°€ ë°”ë€Œê¸° ì „ì—, ì´ì „ì— ìŒ“ì¸ ë°˜ë³µì´ ë§Žìœ¼ë©´ ìš”ì•½ ì¶œë ¥
                if consecutive_count > LOG_REPEAT_LIMIT and printed_lines < LOG_MAX_LINES:
                    print(
                        f"   ... [Skipped {consecutive_count - LOG_REPEAT_LIMIT} "
                        f"identical events for {prev_ip} ({prev_decision})] ..."
                    )
                    printed_lines += 1
                prev_ip = ip
                prev_decision = decision
                consecutive_count = 1
            else:
                consecutive_count += 1

            # ì´ë¯¸ ì´ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë¡œê·¸ê°€ ê½‰ ì°¼ìœ¼ë©´ ë”ëŠ” ì•ˆ ì°ìŒ
            if printed_lines >= LOG_MAX_LINES:
                log_truncated = True
                continue

            should_print = False
            if decision == "BLOCKED":
                if consecutive_count <= LOG_REPEAT_LIMIT:
                    should_print = True
            elif i < 5 or (i % 500 == 0):
                should_print = True

            if should_print and printed_lines < LOG_MAX_LINES:
                prob_val = prob if prob is not None else 0.0
                if "fake_files" in str(served_path):
                    action_msg = f"DECEPTION! ({f_name} -> {served_path})"
                else:
                    action_msg = f"Access Granted ({f_name})"

                repeat_tag = (
                    f"(Repeat {consecutive_count})"
                    if consecutive_count > 1 and decision == "BLOCKED"
                    else ""
                )
                print(
                    f"[{split_name}][{i}] {ip} -> {decision} "
                    f"(prob={prob_val:.4f}) | {action_msg} {repeat_tag}"
                )
                printed_lines += 1

        # ë§ˆì§€ë§‰ êµ¬ê°„ì— ë‚¨ì•„ìžˆë˜ ì¤‘ë³µë„ ìš”ì•½
        if consecutive_count > LOG_REPEAT_LIMIT and printed_lines < LOG_MAX_LINES:
            print(
                f"   ... [Skipped {consecutive_count - LOG_REPEAT_LIMIT} "
                f"identical events for {prev_ip} ({prev_decision})] ..."
            )
            printed_lines += 1

        if log_truncated:
            print(f"[Info] Log outputs for '{split_name}' truncated after {LOG_MAX_LINES} lines.\n")

        print(f"\n[+] Scenario '{split_name}' finished")
        print(f"    - Blocked IPs count: {len(engine.block_list)}")
        print(f"    - Current Fake Files: {len(engine.fake_file_queue)}/{MAX_FAKE_FILES}")
        all_logs.extend(decisions_log)

    # ì „ì²´ ë¡œê·¸ ì €ìž¥
    df_dec = pd.DataFrame(all_logs)
    df_dec.to_csv(ART / "test_decisions.csv", index=False)
    
    # ì •ë‹µ y ëª¨ìœ¼ê¸° (ì„ íƒëœ ì‹œë‚˜ë¦¬ì˜¤ë“¤ë§Œ í†µí•©)
    y_list = []
    for split_name in scenario_splits:
        y_path = ART / f"{split_name}_y.parquet"
        if y_path.exists():
            y_df = pd.read_parquet(y_path)
            y_list.append(y_df)

    if y_list:
        df_y_all = pd.concat(y_list, ignore_index=True)
        df_y_all = df_y_all.drop_duplicates("event_id", keep="last")
        df_y_all.to_parquet(ART / "scenario_y.parquet", index=False)

    # ì‹œê°í™”/ë¦¬í¬íŠ¸
    run_visualization(tau_lo, tau_hi)


if __name__ == "__main__":
    valid_scenarios = data_check_before_run()

    if not valid_scenarios:
        print("\n[!] ê¸°ì¤€ì„ í†µê³¼í•œ ì‹œë‚˜ë¦¬ì˜¤ê°€ ì—†ì–´ ì‹¤í–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        exit(1)

    # ë¬¸ì œ ìžˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ëŠ” ìžë™ìœ¼ë¡œ ì œì™¸í•˜ê³ ,
    # valid_scenarios ë§Œ ê°€ì§€ê³  ë¶„ì„/ê·¸ëž˜í”„ ìˆ˜í–‰
    run_on_test_split(valid_scenarios)
