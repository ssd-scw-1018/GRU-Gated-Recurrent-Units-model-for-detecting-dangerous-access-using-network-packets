# build_scenarios_final.py
from pathlib import Path
import numpy as np
import pandas as pd
import json

# -----------------------------
# ê²½ë¡œ ì„¤ì •
# -----------------------------
# ì´ íŒŒì¼(build_scenarios_final.py)ì´ ìˆëŠ” í´ë” = í”„ë¡œì íŠ¸ ë£¨íŠ¸
ROOT_DIR = Path(__file__).resolve().parent

# ì „ì²˜ë¦¬ íŒŒë¼ë¯¸í„° ìœ„ì¹˜
PREPROC_PARAMS_PATH = ROOT_DIR / "preproc_params.json"

# ì›ë³¸ CSV í´ë”ì™€ íŒŒì¼ ì´ë¦„
DATA_DIR   = "data_4_split"
TEST_CSVS  = ("UNSW-NB15_4.csv",)

# í¬íŠ¸ëŠ” ë©”íƒ€ì—ë§Œ ë³´ê´€(í•™ìŠµ ì…ë ¥ X)
INCLUDE_PORT_FEATURES = False

# ì‚¬ìš©í•  ì—´ ì •ì˜ (preprocessing.pyì™€ ë™ì¼)
USE = [
    "srcip","sport","dstip","dsport",
    "proto","state","dur","sbytes","dbytes","sttl","dttl","sloss","dloss","service",
    "Sload","Dload","Spkts","Dpkts","swin","dwin","stcpb","dtcpb","trans_depth",
    "res_bdy_len","Stime","Ltime","Sintpkt","Dintpkt","tcprtt","synack","ackdat",
    "is_sm_ips_ports","ct_state_ttl","ct_flw_http_mthd","is_ftp_login","ct_ftp_cmd",
    "ct_srv_src","ct_srv_dst","ct_dst_ltm","ct_src_ltm","ct_src_dport_ltm",
    "ct_dst_sport_ltm","ct_dst_src_ltm","attack_cat","Label"
]

CAT_COLS   = ["proto","state","service"]
PORT_COLS  = ["sport","dsport"]
TTL_COLS   = ["sttl","dttl"]
SEQ_COLS   = ["stcpb","dtcpb"]
BOOL_COLS  = ["is_sm_ips_ports","is_ftp_login"]
NUM_LOGZ = [
    "dur","sbytes","dbytes","Sload","Dload","Spkts","Dpkts","swin","dwin",
    "trans_depth","res_bdy_len","Sintpkt","Dintpkt","tcprtt","synack","ackdat",
    "sloss","dloss",
    "ct_state_ttl","ct_flw_http_mthd","ct_ftp_cmd",
    "ct_srv_src","ct_srv_dst","ct_dst_ltm","ct_src_ltm",
    "ct_src_dport_ltm","ct_dst_sport_ltm","ct_dst_src_ltm",
]
META_COLS = ["event_id","srcip","sport","dstip","dsport", "f_name"]

# train/valê³¼ ê²¹ì¹˜ì§€ ì•Šê²Œ í•˜ëŠ” test event_id ì˜¤í”„ì…‹
TEST_BASE     = 10_000_000_000  # 1e10ì¯¤ì´ë©´ ì¶©ë¶„

# =========================
# artifacts dir / RNG
# =========================
ART = ROOT_DIR / "artifacts_parquet"
RNG = np.random.default_rng(20241201)  # ì‹œë‚˜ë¦¬ì˜¤ ì¬í˜„ìš© ì‹œë“œ

# =========================
# ê²°ì¸¡ì¹˜ ì‹œë‚˜ë¦¬ì˜¤ìš© ì„¤ì •
# =========================
INJECT_MISSING_VALUES = True
MISSING_RATE_NUM = 0.15   # ìˆ˜ì¹˜í˜• ê°’ ì¤‘ 15%ë¥¼ NaN
MISSING_RATE_TTL = 0.15   # TTL ê°’ ì¤‘ 15%ë¥¼ NaN
MISSING_RATE_CAT = 0.10   # ë²”ì£¼í˜• ê°’ ì¤‘ 10%ë¥¼ NaN

# ê²°ì¸¡ì¹˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì‚¬ìš©í•  ìµœëŒ€ í–‰ ìˆ˜
MAX_MISSING_ROWS = 100_000


def _cast_numeric(df, cols):
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")
    return df

def _log1p_clip_standardize(col_s, clip_val, mu, sd):
    v = pd.to_numeric(col_s, errors="coerce").astype("float64").values
    v = np.minimum(v, clip_val if np.isfinite(clip_val) else v)
    v = np.log1p(np.clip(v, 0, None))
    sd = (sd if sd not in (0, None, 0.0) else 1.0)
    mu = (mu if mu is not None else 0.0)
    return (v - mu) / sd

def _encode_cats_to_int(s, vocab):
    idx = {tok: i+1 for i, tok in enumerate(vocab)}  # 0=UNK
    return s.map(idx).fillna(0).astype("int32")

def _load_concat_csvs(root: Path, files):
    na_tokens = ["-", "--", "None", "none", "NULL", "null", ""]
    frames = []
    for f in files:
        df_i = pd.read_csv(root / f, low_memory=False, na_values=na_tokens)
        if "f_name" not in df_i.columns:
            df_i["f_name"] = Path(f).name   # UNSW-NB15_4.csv ê°™ì€ íƒœê·¸
        use_cols_present = [c for c in USE if c in df_i.columns]
        if "f_name" not in use_cols_present:
            use_cols_present.append("f_name")
        frames.append(df_i[use_cols_present])
    return pd.concat(frames, ignore_index=True)

def _make_meta(df: pd.DataFrame) -> pd.DataFrame:
    keep = [c for c in META_COLS if c in df.columns]
    meta = df[keep].copy()
    for c in ["srcip","dstip","sport","dsport", "f_name"]:
        if c in meta.columns:
            meta[c] = meta[c].astype(str)
    return meta.set_index("event_id")

# =========================
# ê°’ ìˆ˜ì¤€ NaN ì£¼ì… í•¨ìˆ˜
# =========================
def _inject_value_missing(
    df: pd.DataFrame,
    num_rate: float = MISSING_RATE_NUM,
    ttl_rate: float = MISSING_RATE_TTL,
    cat_rate: float = MISSING_RATE_CAT,
    random_state: int = 20241205,
) -> pd.DataFrame:
    """
    ì¼ë¶€ ì—´ì— ê²°ì¸¡ê°’(NaN)ì„ ëœë¤í•˜ê²Œ ì„ì–´ ë„£ëŠ” í•¨ìˆ˜.
    - NUM_LOGZ, TTL_COLS, CAT_COLSì— ëŒ€í•´ ê°ê° ë¹„ìœ¨ë§Œí¼ NaN ì£¼ì….
    - transform_all()ì˜ ê²°ì¸¡ ëŒ€ì‘ ë¡œì§ì´ ì˜ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸ìš©.
    """
    rng = np.random.default_rng(random_state)
    df = df.copy()
    n_rows = len(df)
    if n_rows == 0:
        return df

    # 1) ë¡œê·¸ ìˆ˜ì¹˜í˜• í”¼ì²˜
    for c in NUM_LOGZ:
        if c in df.columns:
            mask = rng.random(n_rows) < num_rate
            df.loc[mask, c] = np.nan

    # 2) TTL í”¼ì²˜
    for c in TTL_COLS:
        if c in df.columns:
            mask = rng.random(n_rows) < ttl_rate
            df.loc[mask, c] = np.nan

    # 3) ë²”ì£¼í˜• í”¼ì²˜
    for c in CAT_COLS:
        if c in df.columns:
            mask = rng.random(n_rows) < cat_rate
            df.loc[mask, c] = np.nan

    print(
        f"[inject_missing] num_rate={num_rate}, ttl_rate={ttl_rate}, "
        f"cat_rate={cat_rate} ë¡œ NaN ì£¼ì… ì™„ë£Œ"
    )
    return df


def transform_all(df: pd.DataFrame, params: dict,
                  make_time_features=True, drop_time_raw=True,
                  keep_attack_cat=False):
    """
    ì „ì²˜ë¦¬ í•¨ìˆ˜ (ê²°ì¸¡ ì¹¼ëŸ¼ ëŒ€ì‘ ë¡œì§ ì¶”ê°€ë¨)
    - ë²”ì£¼í˜• ëˆ„ë½ ì‹œ: ìµœë¹ˆê°’(Mode)ìœ¼ë¡œ ëŒ€ì²´
    - ìˆ˜ì¹˜í˜• ëˆ„ë½ ì‹œ: í›ˆë ¨ ë°ì´í„°ì˜ í‰ê· (Mean) ë˜ëŠ” ê¸°í•˜í‰ê· ìœ¼ë¡œ ëŒ€ì²´
    """
    X = df.copy()

    # íƒ€ê¹ƒ ë¶„ë¦¬
    y = None
    if "Label" in X.columns:
        y = pd.to_numeric(X["Label"], errors="coerce").fillna(0).astype(int)
        X = X.drop(columns=["Label"])
    if ("attack_cat" in X.columns) and (not keep_attack_cat):
        X = X.drop(columns=["attack_cat"])

    # -------------------------------------------------------------------------
    # [Robustness] ê²°ì¸¡ ì¹¼ëŸ¼ ìë™ ë³´ì™„ ë¡œì§ (Missing Column Imputation)
    # -------------------------------------------------------------------------
    
    # 1. ë²”ì£¼í˜• ê²°ì¸¡ ì²˜ë¦¬ (ìµœë¹ˆê°’ ì‚¬ìš©)
    for c in CAT_COLS:
        if c not in X.columns:
            # vocabsì˜ ì²« ë²ˆì§¸ ìš”ì†Œê°€ ìµœë¹ˆê°’(Mode)
            most_freq = params.get("vocabs", {}).get(c, ["unknown"])[0]
            X[c] = most_freq

    # 2. ë¡œê·¸ ìˆ˜ì¹˜í˜• ê²°ì¸¡ ì²˜ë¦¬ (ê¸°í•˜ í‰ê·  ì‚¬ìš©)
    for c in NUM_LOGZ:
        if c not in X.columns:
            mu = params["mu_log"].get(c, 0.0)
            fill_val = np.expm1(mu) # ì—­ì—°ì‚°ìœ¼ë¡œ ì›ë³¸ ìŠ¤ì¼€ì¼ ë³µì›
            X[c] = fill_val

    # 3. TTL ë“± ì¼ë°˜ ìˆ˜ì¹˜í˜• ê²°ì¸¡ ì²˜ë¦¬ (ì‚°ìˆ  í‰ê·  ì‚¬ìš©)
    for c in TTL_COLS:
        if c not in X.columns:
            mu = params["mu_ttl"].get(c, 0.0)
            X[c] = mu

    # 4. ì‹œí€€ìŠ¤/ë¶ˆë¦¬ì–¸ ë“± ê¸°íƒ€ í•„ìˆ˜ ì¹¼ëŸ¼ ì²˜ë¦¬ (0ìœ¼ë¡œ ëŒ€ì²´)
    for c in SEQ_COLS + BOOL_COLS + ["sbytes", "dbytes", "Spkts", "Dpkts", "swin", "dwin"]:
        if c not in X.columns:
            X[c] = 0
    # -------------------------------------------------------------------------

    # ì´í›„ ê¸°ì¡´ ì „ì²˜ë¦¬ ë¡œì§ ìˆ˜í–‰
    _cast_numeric(X, NUM_LOGZ + TTL_COLS + SEQ_COLS)
    for c in BOOL_COLS:
        if c in X.columns:
            X[c] = pd.to_numeric(X[c], errors="coerce").fillna(0).astype(int)
        else:
            X[c] = 0

    # ë²”ì£¼í˜• â†’ ì •ìˆ˜ ì¸ì½”ë”©(UNK=0)
    for c in CAT_COLS:
        if c in X.columns:
            vocab = params.get("vocabs", {}).get(c, [])
            X[c] = _encode_cats_to_int(X[c].astype(str), vocab)

    # ìˆ˜ì¹˜í˜•: clipâ†’log1pâ†’z
    for c in NUM_LOGZ:
        if c in X.columns:
            X[c] = _log1p_clip_standardize(
                X[c],
                params["p99"].get(c, np.inf),
                params["mu_log"].get(c, 0.0),
                params["sd_log"].get(c, 1.0),
            )

    # TTL: zë§Œ
    for c in TTL_COLS:
        if c in X.columns:
            mu = params["mu_ttl"].get(c, 0.0)
            sd = params["sd_ttl"].get(c, 1.0) or 1.0
            X[c] = (X[c].astype("float64") - mu) / sd

    # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
    if set(SEQ_COLS).issubset(X.columns):
        X["seq_diff"] = (X["stcpb"].astype("float64") - X["dtcpb"].astype("float64"))
    if {"sbytes","dbytes"}.issubset(X.columns):
        X["bytes_tot"]   = X["sbytes"] + X["dbytes"]
        X["bytes_ratio"] = (X["sbytes"] / (X["bytes_tot"] + 1e-6)).clip(0, 1)
    if {"Spkts","Dpkts"}.issubset(X.columns):
        X["pkts_tot"]   = X["Spkts"] + X["Dpkts"]
        X["pkts_ratio"] = (X["Spkts"] / (X["pkts_tot"] + 1e-6)).clip(0, 1)
    if {"swin","dwin"}.issubset(X.columns):
        X["win_ratio"] = (X["swin"] / (X["swin"] + X["dwin"] + 1e-6)).clip(0, 1)
    if {"sttl","dttl"}.issubset(X.columns):
        X["ttl_diff"] = X["sttl"] - X["dttl"]

    # ì‹œê°„ íŒŒìƒ
    if make_time_features and ("Stime" in X.columns):
        ts = pd.to_datetime(pd.to_numeric(X["Stime"], errors="coerce"), unit="s", utc=True)
        X["hour"] = ts.dt.hour.fillna(0).astype("int16")
        X["dow"]  = ts.dt.dayofweek.fillna(0).astype("int8")

    # í•™ìŠµ ì…ë ¥ì—ì„œ ì œì™¸í•  ì¹¼ëŸ¼ ë“œë¡­
    drop_cols = []
    if drop_time_raw:
        drop_cols += [c for c in ["Stime","Ltime"] if c in X.columns]
    drop_cols += [c for c in ["srcip","sport","dstip","dsport", "f_name"] if c in X.columns]
    drop_cols += [c for c in SEQ_COLS if c in X.columns]
    if not INCLUDE_PORT_FEATURES:
        pass
    X = X.drop(columns=drop_cols, errors="ignore")
    X = X.drop(columns=[c for c in ["bytes_tot","pkts_tot"] if c in X.columns], errors="ignore")
    X = X.replace([np.inf, -np.inf], 0.0).fillna(0.0)
    
    return X, y

def prepare_test_parquets():
    """
    preprocessing.pyì˜ transform_holdoutê³¼ ê°™ì€ ì—­í• :
    - UNSW-NB15_4.csv -> test_X/Y/meta.parquet ìƒì„±
    """
    root_dir = ROOT_DIR
    data_dir = root_dir / DATA_DIR
    out_dir  = ART
    out_dir.mkdir(parents=True, exist_ok=True)

    df = _load_concat_csvs(data_dir, TEST_CSVS)
    df["event_id"] = np.arange(len(df), dtype=np.int64) + TEST_BASE
    df = df.sort_values(
        [c for c in ["srcip","Stime","Ltime","event_id"] if c in df.columns],
        kind="mergesort"
    ).reset_index(drop=True)

    meta = _make_meta(df)

    # preproc_params.jsonì€ ëª¨ë¸(.pth)ê³¼ ë™ì¼í•œ í´ë”ì—ì„œ ì½ìŒ
    with open(PREPROC_PARAMS_PATH, "r", encoding="utf-8") as f:
        params = json.load(f)

    Xte, yte = transform_all(df, params, True, True)

    Xte.to_parquet(out_dir / "test_X.parquet",index=False)
    if "Label" in df.columns:
        pd.DataFrame({"event_id": df["event_id"].values, "Label": yte.values})\
          .to_parquet(out_dir / "test_y.parquet", index=False)
    meta.to_parquet(out_dir / "test_meta.parquet")

    print("Test saved:", out_dir.resolve(), "test_X:", Xte.shape)


def _load_test_joined():
    """
    test_X, test_y, test_metaë¥¼ í•œ ë²ˆ í•©ì³ì„œ í° dfë¡œ ë§Œë“  ë’¤ ë°˜í™˜.
    df_all: event_id, [features...], Label, srcip, sport, dstip, dsport, f_name ...
    """
    X    = pd.read_parquet(ART / "test_X.parquet")        # event_id + features
    y    = pd.read_parquet(ART / "test_y.parquet")        # event_id, Label
    meta = pd.read_parquet(ART / "test_meta.parquet")     # index=event_id or col

    if meta.index.name != "event_id":
        meta = meta.set_index("event_id")

    meta_reset = meta.reset_index()

    df = (
        X.merge(y, on="event_id", how="left")
         .merge(meta_reset, on="event_id", how="left")
    )
    X_cols    = X.columns.tolist()          # event_id + featureë“¤
    meta_cols = meta_reset.columns.tolist() # event_id, srcip, dstip, ...
    return df, X_cols, meta_cols


# -------------------------------------------------------------
# ì˜µì…˜ 1 êµ¬í˜„: "ì• ë§¤í•œ ê³µê²©(soft attack)"ì„ ì‹œë‚˜ë¦¬ì˜¤ì— ì£¼ì…
# -------------------------------------------------------------
def _inject_soft_attacks(
    scen_df: pd.DataFrame,
    df_all: pd.DataFrame,
    X_cols,
    soft_ratio: float = 0.1,
) -> pd.DataFrame:
    pos = scen_df[scen_df["Label"] == 1]
    neg_global = df_all[df_all["Label"] == 0]

    if pos.empty or neg_global.empty:
        print("[soft] ê³µê²© ë˜ëŠ” ì •ìƒ ìƒ˜í”Œì´ ë¶€ì¡±í•´ soft attackì„ ë§Œë“¤ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return scen_df

    n_soft = int(len(pos) * soft_ratio)
    n_soft = min(n_soft, len(pos), len(neg_global))
    if n_soft <= 0:
        print("[soft] soft attack ê°œìˆ˜ê°€ 0ì…ë‹ˆë‹¤.")
        return scen_df

    feature_cols = [c for c in X_cols if c != "event_id"]

    atk_idx = RNG.choice(len(pos), size=n_soft, replace=False)
    norm_idx = RNG.choice(len(neg_global), size=n_soft, replace=False)

    pos_sample = pos.iloc[atk_idx].reset_index(drop=True)
    neg_sample = neg_global.iloc[norm_idx].reset_index(drop=True)

    new_rows = []
    base_eid = int(scen_df["event_id"].max()) + 1

    for i in range(n_soft):
        atk_row = pos_sample.iloc[i].copy()
        nor_row = neg_sample.iloc[i]

        alpha = RNG.uniform(0.3, 0.8)

        mixed_feats = (
            alpha * atk_row[feature_cols].to_numpy(dtype=float)
            + (1.0 - alpha) * nor_row[feature_cols].to_numpy(dtype=float)
        )

        atk_row[feature_cols] = mixed_feats
        atk_row["event_id"] = base_eid + i
        atk_row["Label"] = 1

        new_rows.append(atk_row)

    soft_df = pd.DataFrame(new_rows)
    out_df = pd.concat([scen_df, soft_df], axis=0).reset_index(drop=True)

    print(
        f"[soft] soft attacks added: {len(soft_df)} "
        f"(orig pos={len(pos)}, new total rows={len(out_df)})"
    )

    return out_df


# -------------------------------------------------------------------
# ì‹œë‚˜ë¦¬ì˜¤ 1: ì™¸ë¶€ DDoS + ë‹¤ì–‘í•œ ê³µê²©ì IP
# -------------------------------------------------------------------
def _build_ddos_scenario(df_all: pd.DataFrame, X_cols) -> pd.DataFrame:
    pos = df_all[df_all["Label"] == 1].copy()
    neg = df_all[df_all["Label"] == 0].copy()

    if pos.empty:
        raise ValueError("Label=1 (attack) ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. ddos ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë¶ˆê°€.")

    atk_counts = (
        pos.groupby("srcip")["event_id"]
        .count()
        .sort_values(ascending=False)
    )

    main_atk_ip = atk_counts.index[0]
    sub_atk_ips = [ip for ip in atk_counts.index if ip != main_atk_ip][:3]

    src_label_stats = df_all.groupby("srcip")["Label"].agg(["sum", "count"])
    benign_ips = src_label_stats[src_label_stats["sum"] == 0] \
                    .sort_values("count", ascending=False) \
                    .head(30).index.tolist()
    if len(benign_ips) < 5:
        tmp = src_label_stats.copy()
        tmp["neg_ratio"] = (tmp["count"] - tmp["sum"]) / (tmp["count"] + 1e-6)
        benign_ips = tmp.sort_values(["neg_ratio", "count"], ascending=False) \
                        .head(30).index.tolist()

    blocks = []

    # A. í‰ì†Œ íŠ¸ë˜í”½
    target_norm_rows = 8_000
    per_ip_max = 600

    norm_frames = []
    for ip in benign_ips:
        df_ip = neg[neg["srcip"] == ip]
        if df_ip.empty: continue
        n = min(per_ip_max, len(df_ip))
        idx = RNG.choice(len(df_ip), size=n, replace=False)
        norm_frames.append(df_ip.iloc[idx])

    if norm_frames:
        df_norm = pd.concat(norm_frames, axis=0)
        if len(df_norm) > target_norm_rows:
            idx = RNG.choice(len(df_norm), size=target_norm_rows, replace=False)
            df_norm = df_norm.iloc[idx]
        df_norm = df_norm.sample(frac=1.0, random_state=42).reset_index(drop=True)
        blocks.append(df_norm)

    # B. ì„œë¸Œ ê³µê²©ì
    for ip in sub_atk_ips:
        df_ip_pos = pos[pos["srcip"] == ip]
        if len(df_ip_pos) < 20: continue
        n = min(len(df_ip_pos), int(RNG.integers(30, 81)))
        idx = RNG.choice(len(df_ip_pos), size=n, replace=False)
        blocks.append(df_ip_pos.iloc[idx].copy())

        if not neg.empty:
            n_norm = int(n * 0.5)
            idx2 = RNG.choice(len(neg), size=n_norm, replace=False)
            blocks.append(neg.iloc[idx2])

    # C. ë©”ì¸ ê³µê²©ì
    df_main_pos = pos[pos["srcip"] == main_atk_ip]
    n_heavy = min(3_000, len(df_main_pos))
    if n_heavy < 200: n_heavy = len(df_main_pos)
    if n_heavy > 0:
        idx = RNG.choice(len(df_main_pos), size=n_heavy, replace=False)
        blocks.append(df_main_pos.iloc[idx].copy())

    scen_df = pd.concat(blocks, axis=0).reset_index(drop=True)

    print("[ddos scenario]")
    print(f"  main_atk_ip : {main_atk_ip}, total rows : {len(scen_df)}")

    scen_df = _inject_soft_attacks(scen_df, df_all, X_cols, soft_ratio=0.3)
    return scen_df


# -------------------------------------------------------------------
# ì‹œë‚˜ë¦¬ì˜¤ 2: ëŠë¦¬ì§€ë§Œ ê¾¸ì¤€í•œ í¬íŠ¸ ìŠ¤ìº” + ì—¬ëŸ¬ ìŠ¤ìºë„ˆ
# -------------------------------------------------------------------
def _build_slow_scan_scenario(df_all: pd.DataFrame, X_cols) -> pd.DataFrame:
    pos = df_all[df_all["Label"] == 1].copy()
    neg = df_all[df_all["Label"] == 0].copy()

    if pos.empty:
        raise ValueError("Label=1 (attack) ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤. slow_scan ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ë¶ˆê°€.")

    atk_stats = pos.groupby("srcip").agg(
        pos_cnt=("event_id", "count"),
        dst_uniq=("dstip", "nunique")
    )
    atk_stats = atk_stats.sort_values(["dst_uniq", "pos_cnt"], ascending=False)

    scanner_ips = atk_stats.index.tolist()
    main_scan_ip = scanner_ips[0]
    sub_scan_ips = scanner_ips[1:3]

    src_label_stats = df_all.groupby("srcip")["Label"].agg(["sum", "count"])
    benign_ips = src_label_stats[src_label_stats["sum"] == 0] \
                    .sort_values("count", ascending=False) \
                    .head(30).index.tolist()
    if len(benign_ips) < 5:
        tmp = src_label_stats.copy()
        tmp["neg_ratio"] = (tmp["count"] - tmp["sum"]) / (tmp["count"] + 1e-6)
        benign_ips = tmp.sort_values(["neg_ratio", "count"], ascending=False) \
                        .head(30).index.tolist()

    blocks = []

    # A. ì˜¤ì „ ì •ìƒ
    target_norm_rows = 5_000
    per_ip_max = 400

    norm_frames = []
    for ip in benign_ips:
        df_ip = neg[neg["srcip"] == ip]
        if df_ip.empty: continue
        n = min(per_ip_max, len(df_ip))
        idx = RNG.choice(len(df_ip), size=n, replace=False)
        norm_frames.append(df_ip.iloc[idx])

    if norm_frames:
        df_norm = pd.concat(norm_frames, axis=0)
        if len(df_norm) > target_norm_rows:
            idx = RNG.choice(len(df_norm), size=target_norm_rows, replace=False)
            df_norm = df_norm.iloc[idx]
        df_norm = df_norm.sample(frac=1.0, random_state=99).reset_index(drop=True)
        blocks.append(df_norm)

    # B. ì„œë¸Œ ìŠ¤ìºë„ˆ
    for ip in sub_scan_ips:
        df_ip_pos = pos[pos["srcip"] == ip]
        if len(df_ip_pos) < 20: continue
        n = min(len(df_ip_pos), int(RNG.integers(50, 151)))
        idx = RNG.choice(len(df_ip_pos), size=n, replace=False)
        blocks.append(df_ip_pos.iloc[idx].copy())

        if not neg.empty:
            n_norm = int(n * 0.5)
            idx2 = RNG.choice(len(neg), size=n_norm, replace=False)
            blocks.append(neg.iloc[idx2])

    # C. ë©”ì¸ ìŠ¤ìºë„ˆ
    df_main_pos = pos[pos["srcip"] == main_scan_ip]
    n_scan = min(2_000, len(df_main_pos))
    if n_scan < 200: n_scan = len(df_main_pos)
    if n_scan > 0:
        idx = RNG.choice(len(df_main_pos), size=n_scan, replace=False)
        blocks.append(df_main_pos.iloc[idx].copy())

    scen_df = pd.concat(blocks, axis=0).reset_index(drop=True)

    print("[slow_scan scenario]")
    print(f"  main_scan_ip : {main_scan_ip}, total rows : {len(scen_df)}")

    scen_df = _inject_soft_attacks(scen_df, df_all, X_cols, soft_ratio=0.3)
    return scen_df


# -------------------------------------------------------------------
# ì‹œë‚˜ë¦¬ì˜¤ 3: í”¼ì²˜/ì—´ ê²°ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤
# -------------------------------------------------------------------
def build_missing_feature_scenario():
    """
    ë°˜í™˜ì€ í•˜ì§€ ì•Šê³ , ë°”ë¡œ artifacts_parquetì— ì €ì¥:
      - scenario_missing_X.parquet
      - scenario_missing_y.parquet
      - scenario_missing_meta.parquet
      - scenario_missing_raw_missing.json   ğŸ‘ˆ (raw ê²°ì¸¡ë¥  ìš”ì•½)
    (í–‰ ìˆ˜ëŠ” MAX_MISSING_ROWS = 100,000 ìœ¼ë¡œ ì œí•œ)
    """
    root_dir = ROOT_DIR
    data_dir = root_dir / DATA_DIR
    out_dir  = ART
    out_dir.mkdir(parents=True, exist_ok=True)

    # 1) ì›ë³¸ í…ŒìŠ¤íŠ¸ CSV ë¡œë“œ + event_id ë¶€ì—¬
    df = _load_concat_csvs(data_dir, TEST_CSVS)
    df["event_id"] = np.arange(len(df), dtype=np.int64) + TEST_BASE

    # ğŸ”¹ 1-1) ê²°ì¸¡ì¹˜ ì‹œë‚˜ë¦¬ì˜¤ëŠ” ìµœëŒ€ 10ë§Œ í–‰ë§Œ ì‚¬ìš©
    if len(df) > MAX_MISSING_ROWS:
        df = df.sample(n=MAX_MISSING_ROWS, random_state=20241210)
        df = df.sort_values("event_id").reset_index(drop=True)
    print(f"[scenario_missing] rows limited to {len(df)} (MAX_MISSING_ROWS={MAX_MISSING_ROWS})")

    # f_name: ê²°ì¸¡ì¹˜ ì‹œë‚˜ë¦¬ì˜¤ ì „ìš© ì´ë¦„ìœ¼ë¡œ ì¬ìƒì„±
    indices = np.arange(len(df))
    df["f_name"] = "UNSW-NB15_4_missing_" + pd.Series(indices).astype(str) + ".csv"
    print(f"[scenario_missing] f_name unique values generated: {len(df)} files.")

    # 2) ë©”íƒ€( srcip, sport, dstip, dsport, f_name ) í™•ë³´
    meta = _make_meta(df)

    # 3) ì¼ë¶€ ì—´ì„ "í†µì§¸ë¡œ" ë“œë¡­í•´ì„œ
    #    transform_allì—ì„œ "ì—´ ìì²´ê°€ ì—†ëŠ”" ìƒí™©ì„ í…ŒìŠ¤íŠ¸
    DROP_NUM_COLS = ["dur", "Sload"]
    DROP_CAT_COLS = ["proto"]
    drop_cols = [c for c in DROP_NUM_COLS + DROP_CAT_COLS if c in df.columns]
    if drop_cols:
        df = df.drop(columns=drop_cols)
        print(f"[missing_scenario] dropped columns: {drop_cols}")

    # 4) ë‚˜ë¨¸ì§€ ì—´ì—ëŠ” ê°’ ìˆ˜ì¤€ì˜ NaNë„ ì„ì–´ ì£¼ì…
    if INJECT_MISSING_VALUES:
        df = _inject_value_missing(df)

    #  4-1) ì›ë³¸(raw) ê¸°ì¤€ ê²°ì¸¡ë¥  ìš”ì•½ â†’ JSONìœ¼ë¡œ ì €ì¥
    raw_missing_rates = df.isna().mean().to_dict()
    raw_missing_json = {
        "scenario": "missing_feature",
        "n_rows": int(len(df)),
        "missing_rate": {col: float(rate) for col, rate in raw_missing_rates.items()},
    }
    raw_json_path = out_dir / "scenario_missing_raw_missing.json"
    with open(raw_json_path, "w", encoding="utf-8") as f:
        json.dump(raw_missing_json, f, indent=2, ensure_ascii=False)
    print(f"[scenario_missing] raw missing summary saved -> {raw_json_path}")

    # 5) í•™ìŠµ ë•Œì™€ ë™ì¼í•œ params ë¡œ transform_all ìˆ˜í–‰
    with open(PREPROC_PARAMS_PATH, "r", encoding="utf-8") as f:
        params = json.load(f)

    Xmiss, ymiss = transform_all(df, params, True, True)

    # 6) ì €ì¥ (ì´ë¦„ë§Œ scenario_missing_* ìœ¼ë¡œ)
    X_path = out_dir / "scenario_missing_X.parquet"
    y_path = out_dir / "scenario_missing_y.parquet"
    m_path = out_dir / "scenario_missing_meta.parquet"

    Xmiss.to_parquet(X_path, index=False)
    if "Label" in df.columns:
        pd.DataFrame({"event_id": df["event_id"].values, "Label": ymiss.values})\
          .to_parquet(y_path, index=False)
    meta.to_parquet(m_path)

    print(f"[+] saved missing_feature scenario:")
    print(f"    X   -> {X_path} shape={Xmiss.shape}")
    print(f"    y   -> {y_path} shape={ymiss.shape}")
    print(f"    meta-> {m_path} shape={meta.shape}")

# -------------------------------------------------------------------
# ê³µí†µ wrapper (ddos / slow_scan)
# -------------------------------------------------------------------
def build_scenario_df(scenario_type: str):
    """
    ë°˜í™˜: (X_scen, y_scen, meta_scen)
    """
    df_all, X_cols, meta_cols = _load_test_joined()

    if "Label" not in df_all.columns:
        raise ValueError("Label ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. yë¥¼ ë§Œë“¤ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    if scenario_type == "ddos":
        scen_df = _build_ddos_scenario(df_all, X_cols)
    elif scenario_type == "slow_scan":
        scen_df = _build_slow_scan_scenario(df_all, X_cols)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹œë‚˜ë¦¬ì˜¤ íƒ€ì…: {scenario_type}")

    # ========================================================
    # f_nameì„ ê°œë³„ ì ‘ê·¼ë§ˆë‹¤ ê³ ìœ í•˜ê²Œ ë³€ê²½í•˜ëŠ” ë¡œì§
    # í˜•ì‹: UNSW-NB15_40.csv, UNSW-NB15_41.csv ...
    # ========================================================
    if "f_name" in scen_df.columns:
        indices = np.arange(len(scen_df))
        scen_df["f_name"] = "UNSW-NB15_4" + pd.Series(indices).astype(str) + ".csv"
        print(f"[{scenario_type}] f_name unique values generated: {len(scen_df)} files.")

    # X_scen / y_scen / meta_scen ë¶„ë¦¬
    X_scen = scen_df[X_cols].copy()
    y_scen = scen_df[["event_id", "Label"]].copy()

    meta_scen = scen_df[meta_cols].copy()
    meta_scen = meta_scen.set_index("event_id")

    return X_scen, y_scen, meta_scen


def main():
    # 1) ë¨¼ì € test_X / test_y / test_meta ìƒì„±
    if not (ART / "test_X.parquet").exists():
        prepare_test_parquets()
    else:
        print("[info] test_X.parquet already exists. Skip test preprocessing.")

    # 2) DDoS / Slow Scan ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
    scenarios = ["ddos", "slow_scan"]

    for name in scenarios:
        X_scen, y_scen, meta_scen = build_scenario_df(name)

        x_path = ART / f"scenario_{name}_X.parquet"
        y_path = ART / f"scenario_{name}_y.parquet"
        m_path = ART / f"scenario_{name}_meta.parquet"

        X_scen.to_parquet(x_path, index=False)
        y_scen.to_parquet(y_path, index=False)
        meta_scen.to_parquet(m_path)

        print(f"[+] saved {name}:")
        print(f"    X   -> {x_path} shape={X_scen.shape}")
        print(f"    y   -> {y_path} shape={y_scen.shape}")
        print(f"    meta-> {m_path} shape={meta_scen.shape}")

    # 3) ê²°ì¸¡ì¹˜/ëˆ„ë½ì—´ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
    build_missing_feature_scenario()


if __name__ == "__main__":
    main()
